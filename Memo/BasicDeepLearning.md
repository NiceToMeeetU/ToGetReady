# BasicDeepLearning

[TOC]



## NLP相关

### 简单说一下 TF-IDF ？

用词语的重要程度和独特性来描述文档











## CV相关



## 深度学习模型训练



## 优化方法











### 深度学习模型如何提高泛化能力、减少过拟合、提升鲁棒性：

- 设置小一点的`batch_size`；
- 增加模型深度；
- 正则化、BatchNormalize；
- 提早结束训练；
- dropout；
- 数据增强。

### 深度学习中有哪些防止过拟合的方法？

- 从数据入手，尽可能扩增数据，如图像数据增强，通过GAN等补充数据；
- 正则化
- Batch Normalization，
- 设置合适的早停策略，提前结束训练。

### 为什么Dropout可以解决过拟合？

（1）取平均的作用： 因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。

（2）减少神经元之间复杂的共适应关系： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。
————————————————
版权声明：本文为CSDN博主「小小小绿叶」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/litt1e/article/details/106155416

### 讲讲对Transformer和卷积的理解？



### 近年来卷积神经网络的发展历程？

- 最早期Lecun提出的LeNet用于手写字符识别，初步展示了卷积池化全连接的网络结构的效能。但受制于数据量和计算瓶颈，沉寂了近20年；
- 直到12年出现了首个强实用性的卷积网络，AlexNet，取得了远超其他传统方法的识别准确率，开启了新纪元。进步在于采用RELU作为激活函数、引入了Dropout机制、通过分组卷积突破显存瓶颈等；
- 14年VGGNet继续进一步降低了图像分类任务的村无论，改进在于用多个3×3卷积核代替5×5、7×7的大卷积核，更少参数量、更小计算量，但获得了同样的感受野及更大的网络深度；
- GooLeNet即Inception-v1深入探索了网络结构的设计原则，虽然网络性能和表达能力正相关于网络深度，但深度增加会带来overfit、梯度消失等副作用。那么将当前网络中的全连接和卷积等密集连接结构转化为系数连接结构，即可保持深度同时降低计算量。提出Inception模块，将大通道卷积层替换为多个小通道卷积层。包括1×1的卷积来升降维，多个尺寸同时卷积再聚合等；
- Inception-v2/v3进一步在细节上优化；
- 然后就是何恺明大神的ResNet横空出世，引入残差模块，通过跳层短路连接解决网络退化的问题，

### FPN

- 自下而上的特征提取主干网络
  - 以ResNet为基础不断卷积
  - 前馈过程，经过某些层feature map大小不变的归为一个stage，如此形成一层层的特征金字塔；
  - 最底层的结果文中考虑实际语义需求和计算成本舍弃不用
- 自上而下的上采样网络
  - 上层的feature map作2近邻上采样，与下层的特征直接相加
- 横向连接
  - 1x1 conv：减少卷积核个数，融合各个channel信息并降低维度，使bottom-up对应层降维至256，有效缓冲，防止梯度影响到bottom-up主干网路；

### 牛顿法的特点，为什么不能应用到深度学习领域

梯度下降法是一阶优化算法，牛顿法是二阶优化算法
牛顿法的收敛速度相比梯度下降法常常较快，但是计算开销大，实际中常用拟牛顿法
牛顿法对初始值有一定要求，在非凸优化问题中（如神经网络训练），牛顿法很容易陷入鞍点（牛顿法步长会越来越小），而梯度下降法则很容易逃离鞍点（因此在神经网络训练中一般使用梯度下降法，高维空间的神经网络中存在大量鞍点）
梯度下降法在靠近最优点时会震荡，因此步长调整在梯度下降法中是必要的，具体有adagrad, adadelta, rmsprop, adam等一系列自适应学习率的方法