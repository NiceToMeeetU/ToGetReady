### 深度学习模型如何提高泛化能力、减少过拟合、提升鲁棒性：

- 设置小一点的`batch_size`；
- 增加模型深度；
- 正则化、BatchNormalize；
- 提早结束训练；
- dropout；
- 数据增强。



### 讲讲对Transformer和卷积的理解？



### 近年来卷积神经网络的发展历程？

- 最早期Lecun提出的LeNet用于手写字符识别，初步展示了卷积池化全连接的网络结构的效能。但受制于数据量和计算瓶颈，沉寂了近20年；
- 直到12年出现了首个强实用性的卷积网络，AlexNet，取得了远超其他传统方法的识别准确率，开启了新纪元。进步在于采用RELU作为激活函数、引入了Dropout机制、通过分组卷积突破显存瓶颈等；
- 14年VGGNet继续进一步降低了图像分类任务的村无论，改进在于用多个3×3卷积核代替5×5、7×7的大卷积核，更少参数量、更小计算量，但获得了同样的感受野及更大的网络深度；
- GooLeNet即Inception-v1深入探索了网络结构的设计原则，虽然网络性能和表达能力正相关于网络深度，但深度增加会带来overfit、梯度消失等副作用。那么将当前网络中的全连接和卷积等密集连接结构转化为系数连接结构，即可保持深度同时降低计算量。提出Inception模块，将大通道卷积层替换为多个小通道卷积层。包括1×1的卷积来升降维，多个尺寸同时卷积再聚合等；
- Inception-v2/v3进一步在细节上优化；
- 然后就是何恺明大神的ResNet横空出世，引入残差模块，通过跳层短路连接解决网络退化的问题，

