# 机器学习基础

### 常见的模型评估指标有哪些，分别有何局限性？

- 准确率

  容易受到样本不平衡导致的假高

### 如何绘制ROC曲线？

一般通过设置连续的阈值截断点，以假阳率为横坐标，真阳率为纵坐标绘制一系列点，再按照从左到右，从下到上的顺序依次连接即可。

更直观的说法：对一个二分类任务，假定其样本中有P个正例，N个负例，将所有样本通过模型计算出分类概率，按照概率结果从大到小的顺序排列依次遍历，从坐标轴的（0，0）点开始，每遇到一个正样本，就向Y轴方向移动1/P距离，遇到一个负样本，就向X轴方向移动1/N距离，一直到达（1，1）点，即完成ROC曲线的绘制。

ROC曲线横坐标是假阳率，纵坐标是真阳率，所以也可以叫做真阳-假阳曲线，描述了模型的敏感性与特异性的平衡，曲线越接近y=1说明模型效果越好。

### ROC曲线相比PR曲线有何特点？

- 两者都是通过将阈值从高到低依次阶梯状设置后绘制成对的评估指标后依次连接形成的；
- 两者都能反映模型敏感性与特异性的平衡关系；
- ROC曲线是真阳率-假阳率，PR曲线是精确率-召回率；
- ROC曲线由西南到东北，PR曲线一般由西至东南；

当正负样本分布发生变化时，ROC曲线的形状能基本保持不变，而P-R曲线一般会发生明显的形态变化，所以ROC曲线能更稳定的反映模型本身的好坏。

### 如何解决过拟合问题？

是什么？怎样做？实际经验？

模型太过复杂泛化性能差；增大数据量，降低复杂度，参数加正则，多集成学习；暂无

### 为什么增加正则项、减少复杂度就能避免过拟合？

### 为什么减少模型的复杂度就能解决过拟合问题？



### 如何解决欠拟合问题？

是什么？怎样做？实际经验？

模型不能有效拟合数据；添加新特征，提升复杂度，减小正则化。

### 如何看待余弦距离和欧氏距离的关系，还用过哪些距离度量？



### 超参数调优有哪些方法？



---

### 有哪些集成学习方法，区别？

- 基分类器训练方式：

  Boosting串行训练，Bagging并行训练；

- 基本思路：

  Boosting层层叠加learning from mistakes，Bagging分而治之；

- 提高基分类器性能的本质：

  Boosting主要消除偏差，Bagging有效降低方差（故基分类器最好是本身对样本分布较为敏感的不稳定的）

  一直强调基分类器之间的随机性关联性，Boosting的训练过程使得各弱分类器之间强相关、缺乏独立性，故并不会降低方差；而Bagging一直考虑通过各种方法来增加独立性，从而有效降低方差。

### 集成学习常用什么基分类器，为什么？

最常使用决策树作为基分类器：

- 决策树可以较方便地将样本的权重整合到训练过程中，而不需要使用过采样的方法；
- 可以通过调节树的层数来折中平衡决策树的表达能力和泛化能力；
- 数据样本的扰动对决策树影响较大，故不同子样本集合生成的决策树基分类器随机性较大。

神经网络也可做基分类器，同样不稳定，且方便调节层数、权值等引入随机性。

### GBDT的基本原理，优缺点

梯度提升决策树(Gradient Boosting Decision Tree)，基于决策树预测的残差进行迭代的学习。

- 基本思想

  根绝当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。

- 优点

  - 预测阶段计算速度快，树间可并行；
  - 分布稠密的数据集上，泛化性能和表达能力都很好；
  - 采用决策树作为基分类器可具有很好的解释性和鲁棒性，能够自动发现特征间的高阶关系，不需要对数据归一化等；

- 缺点：

  - 训练需要串行，速度慢；
  - 高维稀疏数据集上表现较差；
  - 解决文本分类特征问题上优势不明显；

### XGBoost大魔王

原始的GBDT算法基于经验损失函数的负梯度来构造决策树，只是在决策树构建完成后再进行剪枝。而XGBoost再决策树构建阶段就加入了正则项；

XGBoost本质就是机器学习算法GBDT的高效工程实现：

- 显式地加入了正则项来控制模型复杂度；
- 训练过程对cost函数二阶泰勒展开，同时利用一阶二阶信息；
- 相比于GBDT可以支持除CART之外的多种基分类器；
- 每次迭代可以不用全部数据，而是类似随机森林，支持对数据进行采样；
- 能够自动学习缺失值的处理策略。

### SVM支持向量机的支持向量究竟是什么？



### 逻辑回归与线性回归有何区区别？

逻辑回归是分类问题，线性回归是回归问题，本质区别。

都是用了极大似然估计来对训练样本建模



### 逻辑回归如何解决多分类问题？



### 决策树有哪些常用启发函数？不同种类的决策树有何区别？

简单直观记忆：

- ID3——最大信息增益，只能用于分类；

  信息增益即经验熵减去经验条件熵

- C4.5——最大信息增益比，只能用于分类；

  信息增益比即信息增益比上取值熵

- CART——最大基尼系数，用于分类和回归。

  基尼系数描述数据的纯度

### 如何对决策树进行剪枝？

剪枝是为了降低决策树模型的复杂度，提高其泛化性能：

- 预剪枝

  在树生成过程中剪枝，方法有：

  - 设置一定的深度阈值；
  - 设置一定的子节点样本数阈值；
  - 设置准确度提升阈值。

  需要经验确定策略，有一定的欠拟合风险；

- 后剪枝

  在树完全生成后进行剪枝，得到的泛化能力会更高，但更费时间。

  代价复杂度剪枝等

### 说说对聚类的认识，讲一下常用的聚类算法？





### python中a==b和a is b有何区别？

- a == b，判断两只是否相等；
- a is b，判断两值是否相等且地址是否相同；
- 一般来说，is 比 = =大，即 a is b 为真 a==b 一定为真；a = = b 为真 a is b不一定为真；
- 也有例外情况，a = float(nan), b=a后，虽然两个都是nan，且 a is b，但是有规定 nan 与任意对象比较哪怕是自己结果都是False，所以此时 a==b 不成立

### 如果项目业务需要强解释性，模型应该如何选择？

黑箱模型即不可用，需要在这三种中选择：

- tree-based model
- logistic regression
- linear kernel svm



### 熟悉哪些降维方法？

降维是为了剔除高维数据中的冗余和噪声，发掘数据内部的特性，从而提升特征表达能力，降低训练复杂度，解决维度爆炸的问题，常用的如PCA主成分分析，LDA线性判别分析等，分别对应应用于无监督任务和监督任务，都是经典的线性方法，能有效解决大部分问题，但也存在一定的局限性，通过非线性的核映射等扩展方法能应用于复杂的非线性问题。

两者有相似的求解思路和过程，分别简单介绍一下

PCA

- PCA通过最大化投影方差，将数据投影到主轴上；
- 降维后方差尽可能大，保留的有效信息尽可能地多。
- 完全无监督学习，完全无参数限制，训练过程不需要任何人为参数设定或经验干预，结果只与数据有关，与用户是完全独立的。
- 变换后的少量维度具有不可解释性，无法具体说明该特征的含义
- 数据`x`的第k个主成分，即其协方差矩阵`Σ`的第k个特征值，将原数据按照第k个特征值对应的特征向量的单位向量进行投影即得降维后的结果。

LDA

- 类内方差小，类间方差大，可以保证降维后便于分类
- 同样是求解散度矩阵的前k个特征值特征向量即可；
- 最大化类间散度实际上优化的是每个类别的中心经过投影后离全局中心的投影足够远

> 本来看葫芦以为都看明白了，翻开蓝书瞅了两眼又迷糊了。















