# 机器学习基础

## GBDT专题

### 简单介绍一下GBDT？

GBDT是一种基于boosting思想的加法模型，训练时采用前向分布算法进行贪婪的学习，每次迭代学习一棵CART来拟合之前 t - 1 棵树的预测结果和训练样本真实值的残差。

### 常用的三大主流GBDT工程实现？

- XGBoost
- LightGBM
- CatBoost，更好地处理类别特征

### GBDT具体使用的是哪种树？分类树和回归树的区别是什么？

GBDT 用 CART树构建 Boosting 模型，不论分类还是回归，都是用的是回归树，因为分类树无法处理连续值，要利用连续的负梯度信息学习。

- CART里分类节点分裂时特征选择用gini, 回归用均方差mse，度量目标是对于划分特征A，对应划分点s两边的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小。
- 2.对于决策树建立后做预测的方式，CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。回归树输出不是类别，采用叶子节点的均值或者中位数来预测输出结果。

### 简单介绍一下XGBoost？

xbg是GBDT算法一种高效的工程实践，基于boosting的集成学习思想，训练时采用前向分布算法进行贪婪学习，每次迭代学习一颗树来你和前 t - 1 棵树的预测结果和训练样本真实值的残差。









### 有尝试过XGBoost和GBDT的区别么？在你的项目上是什么导致了这个区别？

[灵魂拷问，你看过Xgboost原文吗？ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/86816771)

尝试过。XGBoost的精度要比GBDT的高且效率也更高，我认为精度高的最大原因是大部分的CTR特征中，我们会将一些稀疏的离散特征转化为连续特征，会产生很多含有缺失值的稀疏列，导致原始GBDT算法效果不好，而XGBoost会对缺失值做一个特殊的处理，在非分布式的效率更高是因为建树时采用了基于分位数的分割点估计算法。

XGBoost使GBDT算法的一种很好的工程实现，并且再算法上做了一些优化，主要的优化在以下几点：

- 首先XGBoost采用二姐泰勒展开拟合损失函数，可以加速模型收敛，加了一个衰减因子，作为学习率，可以减少加进来的树对于原模型的影响，让树的数量变得更多；
- 在原GBDT模型的基础上加了个正则项，对于树的叶子节点的权重做了约束；
- 增加了在随机森林上常用的col subsample 策略；
- 不需要遍历所有可能的分裂点，提出了一种估计分裂点的算法，在工程上做了一个算法的并发实现。









### GBDT的基本原理，优缺点

梯度提升决策树(Gradient Boosting Decision Tree)，基于决策树预测的残差进行迭代的学习。

- 基本思想

  根绝当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。

- 优点

  - 预测阶段计算速度快，树间可并行；
  - 分布稠密的数据集上，泛化性能和表达能力都很好；
  - 采用决策树作为基分类器可具有很好的解释性和鲁棒性，能够自动发现特征间的高阶关系，不需要对数据归一化等；

- 缺点：

  - 训练需要串行，速度慢；
  - 高维稀疏数据集上表现较差；
  - 解决文本分类特征问题上优势不明显；

### XGBoost大魔王

原始的GBDT算法基于经验损失函数的负梯度来构造决策树，只是在决策树构建完成后再进行剪枝。而XGBoost再决策树构建阶段就加入了正则项；

XGBoost本质就是机器学习算法GBDT的高效工程实现：

- 显式地加入了正则项来控制模型复杂度；
- 训练过程对cost函数二阶泰勒展开，同时利用一阶二阶信息；
- 相比于GBDT可以支持除CART之外的多种基分类器；
- 每次迭代可以不用全部数据，而是类似随机森林，支持对数据进行采样；
- 能够自动学习缺失值的处理策略。

### XGBoost对缺失值是如何处理的？

在普通的GBDT策略中，对于缺失值的处理是先手动对缺失值进行填充，然后当做正常值处理，这样的人工填充会影响数据分布，且没有理论支持。而XGBoost采取的策略是先不处理含有缺失值的样本，先一句正常值数据计算特征的分割点，然后再遍历每个分割点的时候，尝试将缺失样本划入左子树和右子树，选择使损失最优的情况。

### XGBoost调参有哪些经验？



### 简单讲讲XGBoost？





---

## 模型评估

### 常见的模型评估指标有哪些，分别有何局限性？

- 准确率

  容易受到样本不平衡导致的假高

### 如何绘制ROC曲线？

一般通过设置连续的阈值截断点，以假阳率为横坐标，真阳率为纵坐标绘制一系列点，再按照从左到右，从下到上的顺序依次连接即可。

更直观的说法：对一个二分类任务，假定其样本中有P个正例，N个负例，将所有样本通过模型计算出分类概率，按照概率结果从大到小的顺序排列依次遍历，从坐标轴的（0，0）点开始，每遇到一个正样本，就向Y轴方向移动1/P距离，遇到一个负样本，就向X轴方向移动1/N距离，一直到达（1，1）点，即完成ROC曲线的绘制。

ROC曲线横坐标是假阳率，纵坐标是真阳率，所以也可以叫做真阳-假阳曲线，描述了模型的敏感性与特异性的平衡，曲线越接近y=1说明模型效果越好。

### ROC曲线相比PR曲线有何特点？

- 两者都是通过将阈值从高到低依次阶梯状设置后绘制成对的评估指标后依次连接形成的；
- 两者都能反映模型敏感性与特异性的平衡关系；
- ROC曲线是真阳率-假阳率，PR曲线是精确率-召回率；
- ROC曲线由西南到东北，PR曲线一般由西至东南；

当正负样本分布发生变化时，ROC曲线的形状能基本保持不变，而P-R曲线一般会发生明显的形态变化，所以ROC曲线能更稳定的反映模型本身的好坏。

### 如何解决过拟合问题？

是什么？怎样做？实际经验？

模型太过复杂泛化性能差；增大数据量，降低复杂度，参数加正则，多集成学习；暂无

### 为什么增加正则项、减少复杂度就能避免过拟合？

### 为什么减少模型的复杂度就能解决过拟合问题？



### 如何解决欠拟合问题？

是什么？怎样做？实际经验？

模型不能有效拟合数据；添加新特征，提升复杂度，减小正则化。

### 如何看待余弦距离和欧氏距离的关系，还用过哪些距离度量？



### 超参数调优有哪些方法？



---

### 有哪些集成学习方法，区别？

- 基分类器训练方式：

  Boosting串行训练，Bagging并行训练；

- 基本思路：

  Boosting层层叠加learning from mistakes，Bagging分而治之；

- 提高基分类器性能的本质：

  Boosting主要消除偏差，Bagging有效降低方差（故基分类器最好是本身对样本分布较为敏感的不稳定的）

  一直强调基分类器之间的随机性关联性，Boosting的训练过程使得各弱分类器之间强相关、缺乏独立性，故并不会降低方差；而Bagging一直考虑通过各种方法来增加独立性，从而有效降低方差。

### 集成学习常用什么基分类器，为什么？

最常使用决策树作为基分类器：

- 决策树可以较方便地将样本的权重整合到训练过程中，而不需要使用过采样的方法；
- 可以通过调节树的层数来折中平衡决策树的表达能力和泛化能力；
- 数据样本的扰动对决策树影响较大，故不同子样本集合生成的决策树基分类器随机性较大。

神经网络也可做基分类器，同样不稳定，且方便调节层数、权值等引入随机性。

- 

### SVM支持向量机的支持向量究竟是什么？



### 逻辑回归与线性回归有何区区别？

逻辑回归是分类问题，线性回归是回归问题，本质区别。

都是用了极大似然估计来对训练样本建模



### 逻辑回归如何解决多分类问题？



### 决策树有哪些常用启发函数？不同种类的决策树有何区别？

简单直观记忆：

- ID3——最大信息增益，只能用于分类；

  信息增益即经验熵减去经验条件熵

- C4.5——最大信息增益比，只能用于分类；

  信息增益比即信息增益比上取值熵

- CART——最大基尼系数，用于分类和回归。

  基尼系数描述数据的纯度

### 如何对决策树进行剪枝？

剪枝是为了降低决策树模型的复杂度，提高其泛化性能：

- 预剪枝

  在树生成过程中剪枝，方法有：

  - 设置一定的深度阈值；
  - 设置一定的子节点样本数阈值；
  - 设置准确度提升阈值。

  需要经验确定策略，有一定的欠拟合风险；

- 后剪枝

  在树完全生成后进行剪枝，得到的泛化能力会更高，但更费时间。

  代价复杂度剪枝等

### 说说对聚类的认识，讲一下常用的聚类算法？





### python中a==b和a is b有何区别？

- a == b，判断两只是否相等；
- a is b，判断两值是否相等且地址是否相同；
- 一般来说，is 比 = =大，即 a is b 为真 a==b 一定为真；a = = b 为真 a is b不一定为真；
- 也有例外情况，a = float(nan), b=a后，虽然两个都是nan，且 a is b，但是有规定 nan 与任意对象比较哪怕是自己结果都是False，所以此时 a==b 不成立

### 如果项目业务需要强解释性，模型应该如何选择？

黑箱模型即不可用，需要在这三种中选择：

- tree-based model
- logistic regression
- linear kernel svm



### 熟悉哪些降维方法？

降维是为了剔除高维数据中的冗余和噪声，发掘数据内部的特性，从而提升特征表达能力，降低训练复杂度，解决维度爆炸的问题，常用的如PCA主成分分析，LDA线性判别分析等，分别对应应用于无监督任务和监督任务，都是经典的线性方法，能有效解决大部分问题，但也存在一定的局限性，通过非线性的核映射等扩展方法能应用于复杂的非线性问题。

两者有相似的求解思路和过程，分别简单介绍一下

PCA

- PCA通过最大化投影方差，将数据投影到主轴上；
- 降维后方差尽可能大，保留的有效信息尽可能地多。
- 完全无监督学习，完全无参数限制，训练过程不需要任何人为参数设定或经验干预，结果只与数据有关，与用户是完全独立的。
- 变换后的少量维度具有不可解释性，无法具体说明该特征的含义
- 数据`x`的第k个主成分，即其协方差矩阵`Σ`的第k个特征值，将原数据按照第k个特征值对应的特征向量的单位向量进行投影即得降维后的结果。

LDA

- 类内方差小，类间方差大，可以保证降维后便于分类
- 同样是求解散度矩阵的前k个特征值特征向量即可；
- 最大化类间散度实际上优化的是每个类别的中心经过投影后离全局中心的投影足够远

> 本来看葫芦以为都看明白了，翻开蓝书瞅了两眼又迷糊了。

### 用过聚类吗？熟悉哪些聚类方法，各有什么优缺点？

聚类的本质还是要求达到类内高相似、类间高差异

- 最常用的k-means

  样本随机选k个点作为每一类的中心点，计算剩下各点到这些中心点的距离（当然这里的距离是广义距离，不一定是欧式的），将各点分配到距离最近的中心点，归属该类。再重新计算每一类新的中心点的位置，重复迭代，直到满足收敛条件；

  优点是直观简单，缺点是需要指定聚类的簇数，设置的不同的初始聚类中心会导致不稳定的结果。完全不适用于非凸数据和环形数据

- 层次聚类

  自下而上：每个样本点都作为一类，计算所有点两两之间的类间距离，将最近的两个合并醉成新的类，确定新的类中心。重复该步骤迭代直至满足条件。

  缺点：计算量巨大，迭代终止条件难以确定；

- 混合高斯模型

- 加上各类核方法





### 生成式模型和判别式模型有何区别？

假设可观测到的变量集合是`X`，需要预测的变量集合是`Y`，其他的变量集合是`Z`。

- 生成式模型：
  - 对==联合分布概率==`P(X,Y,Z)`进行建模，在给定观测集合`X`的条件下，通过计算边缘分布来得到对变量集合`Y`的推断
  - 关注数据如何生成，能够反映同类数据本身的相似度，不关心划分不同类的边界究竟在哪；
  - 常见：朴素贝叶斯、贝叶斯网络、混合高斯、KNN、隐马尔科夫HMM、马尔科夫随机场、LDA、深度信念网络；
- 判别式模型：
  - 直接对==条件概率==`P(Y,Z|X)`或决策函数`Y=f(X)`进行建模，然后消掉无关变量`Z`得到条件概率;
  - 关注类别间的差异，不能反映训练数据本身的特性，而是去寻找不同类别的最优的差异；
  - 常见：线性回归、逻辑回归、线性判别分析、支持向量机、CART、神经网络、高斯过程、条件随机场；
- 两种模型都是使后验概率最大化，判别式直接对后验概率建模，生成式通过贝叶斯定理将问题转化为求联合概率。
- 面试的时候用汉语、英语、法语、俄语的例子说明。

![preview](http://wy-typora-img.oss-cn-chengdu.aliyuncs.com/img/v2-c052779ddaa1b9d854658e161d9d2509_r.jpg)

